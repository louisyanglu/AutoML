{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>变量基本信息</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_info(train):\n",
    "    \"\"\"\n",
    "    返回 DataFrame\n",
    "    Args:\n",
    "        train (_type_): DataFrame\n",
    "    \"\"\"\n",
    "    base_info = []\n",
    "    for col in train.columns:\n",
    "        base_info.append((col,  \n",
    "                        train[col].nunique(), \n",
    "                        train[col].isnull().sum() * 100 / train.shape[0], \n",
    "                        train[col].value_counts(normalize=True, dropna=False).values[0] * 100, \n",
    "                        train[col].dtype))\n",
    "    base_info_df = pd.DataFrame(\n",
    "        base_info, columns=['Feature', 'unique值个数', '缺失率', '最大同值率', 'type'])\n",
    "    base_info_df.sort_values('缺失率', ascending=False)\n",
    "    return base_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>缺失值分布图</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing(train):\n",
    "    \"\"\"\n",
    "    缺失值分布\n",
    "    \"\"\"\n",
    "    missing = train.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if missing.shape[0] > 0:\n",
    "        missing.sort_values(ascending=False, inplace=True)\n",
    "        missing.plot.bar()\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>单变量分布（连续变量）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 箱形图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(train_data):\n",
    "    \"\"\"\n",
    "    箱形图:主要借助中位数和四分位数来进行计算，以上四分位数+1.5倍四分位距为上界、下四分位数-1.5倍四分位距为下界，超出界限则认为是异常值\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(train_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        sns.boxplot(x=train_data[col], width=0.5, ax=ax)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "孤立森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_by_isoforest(data, if_plot=True):\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    clf = IsolationForest(contamination=0.01, bootstrap=True, random_state=5)\n",
    "    outlier_index = clf.fit_predict(data)\n",
    "    outliers = data[outlier_index == -1]\n",
    "    if if_plot and data.shape[1] >= 2:\n",
    "        col1 = data.columns[0]\n",
    "        col2 = data.columns[1]\n",
    "        sns.scatterplot(data[col1], data[col2], hue=outlier_index, palette='Set1')\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "距均值3倍标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_by_modelpred(model, X, y, sigma=3):\n",
    "    \"\"\"\n",
    "    样本维度：以样本标签均值-3倍标注差为下界，均值+3倍标准差为上界，来检测是否有超过边界的点\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def plot_outliers(y, y_pred, z, outliers):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        ax_131 = plt.subplot(1, 3, 1)\n",
    "        plt.plot(y, y_pred, '.')\n",
    "        plt.plot(y.loc[outliers], y_pred.loc[outliers], 'ro')\n",
    "        plt.legend(['Accepted', 'Outliers'])\n",
    "        plt.xlabel('y')\n",
    "        plt.ylabel('y_pred')\n",
    "        \n",
    "        ax_132 = plt.subplot(1, 3, 2)\n",
    "        plt.plot(y, y_pred, '.')\n",
    "        plt.plot(y.loc[outliers], y.loc[outliers] - y_pred.loc[outliers], 'ro')\n",
    "        plt.legend(['Accepted', 'Outliers'])\n",
    "        plt.xlabel('y')\n",
    "        plt.ylabel('y - y_pred')\n",
    "\n",
    "        ax_133 = plt.subplot(1, 3, 3)\n",
    "        z.plot.hist(bins=50, ax=ax_133)\n",
    "        z.loc[outliers].plot.hist(color='r', bins=50, ax=ax_133)\n",
    "        plt.legend(['Accepted', 'Outliers'])\n",
    "        plt.xlabel('z')\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = pd.Series(model.predict(X), index=y.index)\n",
    "    residuals = y - y_pred\n",
    "    mean_resid = residuals.mean()\n",
    "    std_resid = residuals.std()\n",
    "    # cal z statistic, define outliers to be where |z| > sigma\n",
    "    z = (residuals - mean_resid) / std_resid\n",
    "    outliers = z[abs(z) > sigma].index\n",
    "    plot_outliers(y, y_pred, z, outliers)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核密度估计图\n",
    "训练集、测试集分布是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(train_data, test_data):\n",
    "    \"\"\"\n",
    "    核密度估计图,是在概率论中用来估计未知的密度函数，属于非参数检验的方法之一。\n",
    "    通过核密度估计图可以比较直观地看出数据样本本身的分布特征\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(test_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in test_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        sns.kdeplot(train_data[col], color='red', shade=True)\n",
    "        sns.kdeplot(test_data[col], color='blue', shade=True)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax = ax.legend(['train', 'test'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单变量psi值\n",
    "训练集、测试集分布是否一致  \n",
    "非监督算法（等频、等距）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算psi（传入数据可为变量、分数）\n",
    "def cal_psi(actual, expected, cut_method='equalfrequency', cut_points=[], bins=10):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        actual：实际数据（线上），Series\n",
    "        expected：期望数据（线下），Series\n",
    "        cut_method：分箱方法，'auto'：手动分箱，'equidistance'：等距分箱，'equalfrequency'：等频分箱\n",
    "        cut_points：切点列表\n",
    "        bins：分箱数\n",
    "    return:\n",
    "        psi_info：各区间的分布及psi\n",
    "        psi：psi值\n",
    "    \"\"\"\n",
    "    # 1.确定分箱方法\n",
    "    if cut_method == 'auto':  # 手动分箱\n",
    "        cut_points = [-np.inf] + cut_points + [np.inf]\n",
    "    elif cut_method == 'equidistance':  # 等距分箱\n",
    "        bins = min(bins, expected.nunique())  # 去掉空值后值的个数与给定箱数取小\n",
    "        cut_points = list(np.linspace(expected.min(), expected.max(), num=bins))\n",
    "        cut_points = [-np.inf] + cut_points + [np.inf]\n",
    "    elif cut_method == 'equalfrequency':  # 等频分箱\n",
    "        bins = min(bins, expected.nunique())  # 去掉空值后值的个数与给定箱数取小\n",
    "        cut_points = list(np.nanquantile(expected, [i / bins for i in range(1, bins)])) if bins > 1 \\\n",
    "            else list(np.nanquantile(expected, [i / bins for i in range(1, bins + 1)]))\n",
    "        cut_points = sorted(list(set(cut_points)))\n",
    "        cut_points = [-np.inf] + cut_points + [np.inf]\n",
    "\n",
    "    # 2.计算各区间的样本量\n",
    "    Range = ['missing']\n",
    "    expected_bins_cnt = [expected.isnull().sum()]\n",
    "    actual_bins_cnt = [actual.isnull().sum()]\n",
    "    for i in range(len(cut_points) - 1):\n",
    "        # print(i)\n",
    "        # 区间\n",
    "        left = round(cut_points[i], 2)\n",
    "        right = round(cut_points[i + 1], 2)\n",
    "        Range.append('(' + str(left) + ',' + str(right) + ']')\n",
    "        # 计算expected样本量\n",
    "        ebins_cnt = ((expected > left) & (expected <= right)).sum()\n",
    "        expected_bins_cnt.append(ebins_cnt)\n",
    "        # 计算actual样本量\n",
    "        abins_cnt = ((actual > left) & (actual <= right)).sum()\n",
    "        actual_bins_cnt.append(abins_cnt)\n",
    "    psi_info = pd.DataFrame({'Range': Range, 'Expected_cnt': expected_bins_cnt, 'Actual_cnt': actual_bins_cnt})\n",
    "\n",
    "    # 3.计算psi\n",
    "    psi_info['Expected_pct'] = psi_info['Expected_cnt'] / len(expected)\n",
    "    psi_info['Expected_pct'] = np.where(psi_info['Expected_pct'] == 0, 0.00001, psi_info['Expected_pct'])\n",
    "\n",
    "    psi_info['Actual_pct'] = psi_info['Actual_cnt'] / len(actual)\n",
    "    psi_info['Actual_pct'] = np.where(psi_info['Actual_pct'] == 0, 0.00001, psi_info['Actual_pct'])\n",
    "\n",
    "    psi_info['A-E'] = psi_info['Actual_pct'] - psi_info['Expected_pct']\n",
    "    psi_info['A/E'] = psi_info['Actual_pct'] / psi_info['Expected_pct']\n",
    "    psi_info['ln(A/E)'] = np.log(psi_info['A/E'])\n",
    "\n",
    "    psi_info['PSI'] = psi_info['A-E'] * psi_info['ln(A/E)']\n",
    "    psi = psi_info['PSI'].sum()\n",
    "\n",
    "    return psi, psi_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _psi(base_series, test_series, bins=10, min_sample=10):\n",
    "    base_list = base_series.values\n",
    "    test_list = test_series.values\n",
    "    try:\n",
    "        base_df = pd.DataFrame(base_list, columns=['score'])\n",
    "        test_df = pd.DataFrame(test_list, columns=['score']) \n",
    "        \n",
    "        # 1.去除缺失值后，统计两个分布的样本量\n",
    "        base_notnull_cnt = len(list(base_df['score'].dropna()))\n",
    "        test_notnull_cnt = len(list(test_df['score'].dropna()))\n",
    "        \n",
    "        # 空分箱\n",
    "        base_null_cnt = len(base_df) - base_notnull_cnt\n",
    "        test_null_cnt = len(test_df) - test_notnull_cnt\n",
    "        \n",
    "        # 2.最小分箱数\n",
    "        q_list = []\n",
    "        if pd.api.types.is_numeric_dtype(base_series):  # 连续型，按分位数分箱汇总\n",
    "            if type(bins) == int:\n",
    "                bin_num = min(bins, int(base_notnull_cnt / min_sample))\n",
    "                q_list = [x / bin_num for x in range(1, bin_num)]\n",
    "                break_list = []\n",
    "                for q in q_list:\n",
    "                    bk = base_df['score'].quantile(q)\n",
    "                    break_list.append(bk)\n",
    "                break_list = sorted(list(set(break_list))) # 去重复后排序\n",
    "                score_bin_list = [-np.inf] + break_list + [np.inf]\n",
    "            else:\n",
    "                score_bin_list = bins\n",
    "        elif pd.api.types.is_object_dtype(base_series):\n",
    "                score_bin_list = sorted(list(set(list(base_list) + list(test_list))))  # 离散型，直接按值汇总\n",
    "        \n",
    "        # 4.统计各分箱内的样本量\n",
    "        base_cnt_list = [base_null_cnt]\n",
    "        test_cnt_list = [test_null_cnt]\n",
    "        bucket_list = [\"MISSING\"]\n",
    "        for i in range(len(score_bin_list)-1):\n",
    "            left  = round(score_bin_list[i+0], 4)\n",
    "            right = round(score_bin_list[i+1], 4)\n",
    "            bucket_list.append(\"(\" + str(left) + ',' + str(right) + ']')\n",
    "            \n",
    "            base_cnt = base_df[(base_df.score > left) & (base_df.score <= right)].shape[0]\n",
    "            base_cnt_list.append(base_cnt)\n",
    "            \n",
    "            test_cnt = test_df[(test_df.score > left) & (test_df.score <= right)].shape[0]\n",
    "            test_cnt_list.append(test_cnt)\n",
    "        \n",
    "        # 5.汇总统计结果    \n",
    "        stat_df = pd.DataFrame({\"variable\": base_series.name, \"bucket\": bucket_list, \"base_cnt\": base_cnt_list, \"test_cnt\": test_cnt_list})\n",
    "        stat_df['base_dist'] = stat_df['base_cnt'] / len(base_df)\n",
    "        stat_df['test_dist'] = stat_df['test_cnt'] / len(test_df)\n",
    "        \n",
    "        def sub_psi(row):\n",
    "            # 6.计算PSI\n",
    "            base_list = row['base_dist']\n",
    "            test_dist = row['test_dist']\n",
    "            # 处理某分箱内样本量为0的情况\n",
    "            if base_list == 0 and test_dist == 0:\n",
    "                return 0\n",
    "            elif base_list == 0 and test_dist > 0:\n",
    "                base_list = 1 / base_notnull_cnt\n",
    "            elif base_list > 0 and test_dist == 0:\n",
    "                test_dist = 1 / test_notnull_cnt\n",
    "                \n",
    "            return (test_dist - base_list) * np.log(test_dist / base_list)\n",
    "        \n",
    "        stat_df['psi'] = stat_df.apply(lambda row: sub_psi(row), axis=1)\n",
    "        stat_df = stat_df[['variable', 'bucket', 'base_cnt', 'base_dist', 'test_cnt', 'test_dist', 'psi']]\n",
    "        \n",
    "        psi = stat_df['psi'].sum()\n",
    "        \n",
    "    except:\n",
    "        print('error!!!')\n",
    "        psi = np.nan \n",
    "        stat_df = None    \n",
    "    return psi, stat_df\n",
    "\n",
    "def unpack_tuple(x):\n",
    "        if len(x) == 1:\n",
    "            return x[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "def calculate_psi(base, test, return_frame=True):\n",
    "    psi = list()\n",
    "    frame = list()\n",
    "\n",
    "    if isinstance(test, pd.DataFrame):\n",
    "        for col in test:\n",
    "            p, f = _psi(base[col], test[col])\n",
    "            psi.append(p)\n",
    "            frame.append(f)\n",
    "\n",
    "        psi = pd.Series(psi, index = test.columns)\n",
    "    else:\n",
    "        psi, frame = _psi(base, test)\n",
    "    \n",
    "    res = (psi,)\n",
    "    if return_frame:\n",
    "        res += (frame,)\n",
    "    return unpack_tuple(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KS检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_test(train, test, col, thresh=0.01):\n",
    "    # < 0.01 # 原假设：独立同分布，p越小越拒绝\n",
    "    p_val = ks_2samp(train[col], test[col]).pvalue\n",
    "    if p_val < thresh:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直方图\n",
    "变量是否服从正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(train_data):\n",
    "    \"\"\"\n",
    "    直方图,通过表示沿数据范围形成封箱，然后绘制条形以显示落入每个分箱贯彻的次数的数据分布\n",
    "    结合QQ图,可观察变量是否需要做变换,以更接近正态分布\n",
    "    Args:\n",
    "        train_data (_type_): _description_\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(train_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        # sns.histplot(train_data[col], bins=10, kde=True, ax=ax)\n",
    "        sns.distplot(train_data[col], bins=10, hist=True, kde=True, fit=stats.norm)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QQ图\n",
    "变量是否服从正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qq(train_data):\n",
    "    \"\"\"\n",
    "    绘制QQ图,查看变量是否服从正态分布\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(train_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        stats.probplot(train_data[col], plot=plt)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 直方图 + QQ图\n",
    "变量是否服从正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_qq(train_data):\n",
    "    \"\"\"\n",
    "    画直方图 + QQ图\n",
    "    观察变量是否服从正态分布\n",
    "    Args:\n",
    "        train_data (_type_): 训练集\n",
    "    \"\"\"\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(train_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        # sns.histplot(train_data[col], bins=10, kde=True, ax=ax)\n",
    "        sns.distplot(train_data[col], bins=10, hist=True, kde=True, fit=stats.norm)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        stats.probplot(train_data[col], plot=plt)\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box-Cox变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def box_cox_transform(train, test):\n",
    "    \"\"\"\n",
    "    boxcox变换中x必须为一维的正的数组\n",
    "    y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "        log(x),                  for lmbda = 0\n",
    "    \n",
    "    Args:\n",
    "        train (_type_): _description_\n",
    "        test (_type_): _description_\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    all_scaled = scaler.fit(pd.concat([train[test.columns], test], axis=0))\n",
    "    train_scaled = scaler.transform(train[test.columns])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    train_scaled = pd.DataFrame(train_scaled, columns=test.columns)\n",
    "    test_scaled = pd.DataFrame(test_scaled, columns=test.columns)\n",
    "    \n",
    "    for col in train_scaled.columns:\n",
    "        trans_train_var, lambda_var = stats.boxcox(train_scaled[col].dropna() + 1)\n",
    "        trans_test_var = stats.boxcox(test_scaled[col].dropna() + 1, lmbda=lambda_var)\n",
    "        train[col] = trans_train_var\n",
    "        test[col] = trans_test_var\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性关系图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg(train_data, y_label='target'):\n",
    "    \"\"\"\n",
    "    线性关系图\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(60, int(np.ceil(train_data.columns.shape[0] / 6) * 50)), dpi=75)  # 4:3\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        sns.regplot(x=col, y=y_label, data=train_data, ax=ax, scatter_kws={'marker': '.', 's': 3, 'alpha': 0.3}, line_kws={'color': 'k'})\n",
    "        ax.set_xlabel(col, fontsize=36)\n",
    "        ax.set_ylabel('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>单变量分布（离散变量）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(train_data):\n",
    "    \"\"\"\n",
    "    柱状图\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for col in train_data.columns:\n",
    "        i += 1\n",
    "        ax = plt.subplot(len(train_data.columns), 6, i)\n",
    "        sns.countplot(train_data[col], data=train_data, ax=ax)\n",
    "        ax.set_xlabel(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>双变量分布（连续 & 连续）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 相关性系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(data_train, y_label='target', n_corr=10):\n",
    "    \"\"\"\n",
    "    1.plot_kde,剔除分布不一致的变量\n",
    "    2.计算剩余X与y的相关系数\n",
    "    Args:\n",
    "        data_train (_type_): 训练集\n",
    "        y_label (str, optional): y标签. Defaults to 'target'.\n",
    "        n_corr (int, optional): 挑选最相关的`n`个变量. Defaults to 10.\n",
    "    \"\"\"\n",
    "    train_corr = data_train.corr()\n",
    "    cols_k_largest = train_corr.nlargest(n_corr, y_label)[y_label].index\n",
    "    return cols_k_largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 成对散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pair_scatter(data):\n",
    "    \"\"\"\n",
    "    成对散点图\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "    \"\"\"\n",
    "    sns.pairplot(data, kind=\"reg\", diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(train_corr):\n",
    "    \"\"\"\n",
    "    热力图\n",
    "    Args:\n",
    "        train_corr (_type_): 相关系数矩阵\n",
    "    \"\"\"\n",
    "    sns.heatmap(train_corr, vmax=.8, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>双变量分布（离散 & 离散）</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 柱状图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 堆叠柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_histogram(data, x_label, y_label1, y_label2):\n",
    "    \"\"\"\n",
    "    堆叠柱状图\n",
    "    Args:\n",
    "        data (_type_): DataFrame\n",
    "        x_label (_type_): 变量分箱\n",
    "        y_label1 (_type_): 组1标签\n",
    "        y_label2 (_type_): 组2标签\n",
    "    \"\"\"\n",
    "    s1 = sns.barplot(x=x_label, y=y_label1, data=data, color='red')\n",
    "    s2 = sns.barplot(x=x_label, y=y_label2, data=data, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分组柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_histogram(data, x_label, y_label, group):\n",
    "    \"\"\"\n",
    "    分组柱状图\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        x_label (_type_): 变量分箱\n",
    "        y_label (_type_): 标签\n",
    "        group (_type_): 分组依据\n",
    "    \"\"\"\n",
    "    sns.barplot(x =x_label, y = y_label, data = data, hue = group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>特征处理</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "'''\n",
    "每个样本点都对标准化过程产生影响,适用于有异常值的场景\n",
    "缩放到均值为0,方差为1\n",
    "'''\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "'''\n",
    "对样本进行归一化,映射到[0,1]\n",
    "'''\n",
    "scaler = Normalizer(norm='l2')  #! 按行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "'''\n",
    "映射到[0,1]\n",
    "'''\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "'''\n",
    "映射到[-1,1]\n",
    "'''\n",
    "scaler = MaxAbsScaler()  # x_new = x / max(abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二值化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "'''\n",
    "> 阈值映射为1\n",
    "<= 则映射为0\n",
    "'''\n",
    "scaler = Binarizer(threshold=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 哑变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有序编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()  #! 处理y标签专用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # mean、medain、most_frequent、constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测填充  \n",
    "在每个步骤中,选择一个特征作为输出y,其他所有特征作为输入的X。然后在X和y上训练一个回归器,用来预测y的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "imputer = IterativeImputer(RandomForestRegressor(), random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/img_convert/91be57ac7102e168ef93592d1bc45fb8.png\" alt=\"image-20210815163752767\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 过滤法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 应用场景\n",
    "- 对于回归：\n",
    "1. f_regression 回归任务的标签/功能之间的F值\n",
    "2. mutual_info_regression 共同目标的共同信息\n",
    "\n",
    "- 对于分类：\n",
    "3. chi2 分类任务的非负特征的卡方统计\n",
    "4. f_classif 标签/功能之间的ANOVA F值用于分类任务\n",
    "5. mutual_info_classif 离散目标的相互信息\n",
    "\n",
    "- 其他方法：\n",
    "6. SelectPercentile 根据最高分数的百分位数选择功能。\n",
    "7. SelectFpr 根据误报率测试选择功能。\n",
    "8. SelectFdr 根据估计的错误发现率选择功能。\n",
    "9. SelectFwe 根据家庭错误率选择功能。\n",
    "10. GenericUnivariateSelect 具有可配置模式的单变量特征选择器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法-相关系数法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.,  1.,  0., ...,  9.,  7.,  0.],\n",
       "       [13.,  5.,  0., ...,  3.,  0.,  0.],\n",
       "       [15., 12.,  0., ...,  1.,  0.,  0.],\n",
       "       ...,\n",
       "       [15.,  1.,  0., ...,  4.,  0.,  0.],\n",
       "       [ 7.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "       [ 8.,  1.,  0., ..., 12.,  6.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, r_regression\n",
    "X, y = load_digits(return_X_y=True, as_frame=True)\n",
    "\n",
    "select_corr = SelectKBest(r_regression, k=20)\n",
    "X_new = select_corr.fit_transform(X, y)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_2',\n",
       "       'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_2', 'pixel_2_5',\n",
       "       'pixel_2_6', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5',\n",
       "       'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_5_6', 'pixel_5_7'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_corr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_2',\n",
       "       'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_2', 'pixel_2_5',\n",
       "       'pixel_2_6', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5',\n",
       "       'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_5_6', 'pixel_5_7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[select_corr.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import r_regression, f_regression\n",
    "\n",
    "# pearsonr返回r，相关性系数、p-value（双尾），p值可粗略表示两者不相关的概率，p越小越相关\n",
    "# score_func = lambda X, y: np.array(list(map(lambda x: pearsonr(x, y), X.T))).T[0]  # r值\n",
    "def feature_selection_corr(X_train_temp, X_test_temp, y_train, k):\n",
    "    \"\"\"\n",
    "    相关系数：衡量连续变量的同步变化趋势，很多分类问题采用相关系数进行衡量并不能得到一个很好的结果\n",
    "    在机器学习建模流程中，借助关联度指标进行特征筛选也往往是初筛，并不需要精准的检验结果作为依据，\n",
    "    例如我们并不会以p值作为特征是否可用的依据（并不只使用那些显著相关的变量），\n",
    "    而是“简单粗暴”的划定一个范围（例如挑选相关性前100的特征）\"\"\"\n",
    "    # 1.r_regression只会根据输入函数的评分按照由高到低进行筛选，因此输出的特征只是相关系数最大并不是相关系数绝对值最大的特征，\n",
    "    # 2.r_regression返回结果的特征并未排序\n",
    "    # KB= SelectKBest(r_regression, k=k)\n",
    "    # 3.f_regression计算F-Score，基于F-Score的排序结果和基于相关系数绝对值的排序结果一致\n",
    "    KB= SelectKBest(f_regression, k=k)\n",
    "    KB.fit_transform(X_train_temp, y_train)\n",
    "    selected_var = X_train_temp.columns[KB.get_support()]\n",
    "    # 4.SelectPercentile可以按照比例进行筛选\n",
    "    # SP = SelectPercentile(f_regression, percentile=30)\n",
    "    X_train_temp = X_train_temp[selected_var]\n",
    "    X_test_temp = X_test_temp[selected_var]\n",
    "    return X_train_temp, X_test_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法-方差选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_var(X, threshold=0):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "    select_var = VarianceThreshold(threshold=threshold)\n",
    "    select_var.fit(X)\n",
    "    selected_var = X.columns[select_var.get_support()]\n",
    "    return selected_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法-卡方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''离散变量'''\n",
    "def feature_selection_chi(X, y, threshold=0.05):\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import chi2\n",
    "\n",
    "    chi_X = X.copy()\n",
    "    chi_X = chi_X.where(chi_X >= 0, 0)  # 卡方检验不能非负\n",
    "\n",
    "    chival, pval = chi2(chi_X, y)\n",
    "    k_chi = pval.shape[0] - (pval > threshold).sum()  # 原假设：两者独立无关，p越小越拒绝\n",
    "    select_chi = SelectKBest(chi2, k=k_chi)\n",
    "    select_chi.fit(chi_X, y)\n",
    "    selected_chi = X.columns[select_chi.get_support()]\n",
    "    return selected_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法-ANOVA方差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''连续变量'''\n",
    "def feature_selection_f(X, y, threshold=0.05):\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import f_classif\n",
    "\n",
    "    fval, pval = f_classif(X, y)\n",
    "    k_f = pval.shape[0] - (pval > threshold).sum()\n",
    "    select_f = SelectKBest(f_classif, k=k_f)\n",
    "    select_f.fit(X, y)\n",
    "    selected_f = X.columns[select_f.get_support()]\n",
    "    return selected_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤法-互信息  \n",
    "互信息，可以看成是一个随机变量中包含的关于另一个随机变量的信息量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_mic(X, y, threshold=0):\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import mutual_info_classif as mic\n",
    "\n",
    "    val = mic(X, y)  # 互信息估计值\n",
    "    k_mic = val.shape[0] - (val <= threshold).sum()\n",
    "    select_mic = SelectKBest(mic, k=k_mic)\n",
    "    select_mic.fit(X, y)\n",
    "    selected_mic = X.columns[select_mic.get_support()]\n",
    "    return selected_mic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 包装法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包装法-RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''很慢'''\n",
    "# def feature_selection_wrapper(clf, X, y, scoring='roc_auc', threshold=[], step=1):\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from sklearn.feature_selection import RFE\n",
    "#     from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#     if len(threshold) == 0:\n",
    "#         threshold = np.linspace(1, int(np.ceil(X.shape[1] / 10)) * 9 + 1, 10)\n",
    "#         step = np.ceil(X.shape[1] / 10)\n",
    "#     scores = []\n",
    "#     for th in threshold:\n",
    "#         X_wrapper = RFE(clf, n_features_to_select=int(th),\n",
    "#                         step=step).fit_transform(X, y)\n",
    "#         score = cross_val_score(clf, X_wrapper, y, scoring=scoring, cv=5, n_jobs=-1, error_score='raise').mean()  # cross_val_score也是测试集分数\n",
    "#         scores.append(score)\n",
    "#     plt.plot(threshold, scores)\n",
    "#     plt.show()\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_wrapper(model, X_train, y_train, X_test, y_test, X_oot, y_oot, scoring='roc_auc'):\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    rfe = RFE(model, n_features_to_select=1, step=1, verbose=0)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    dic = {k : v for k, v in zip(rfe.ranking_, X_train.columns)}\n",
    "    feature_ranking = pd.Series(dic).reset_index().rename({'index': 'ranking', 0: 'feature'}, axis=1)\n",
    "    feature_ranking = feature_ranking.sort_values('ranking', axis=0)\n",
    "\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    oot_scores = []\n",
    "    for i in range(1, X_train.columns.shape[0] + 1):\n",
    "        cols = []\n",
    "        for k, v in dic.items():\n",
    "            if k <= i:\n",
    "                cols.append(v)\n",
    "        model.fit(X_train[cols], y_train)\n",
    "        train_score = cross_val_score(model, X_train[cols], y_train, scoring=scoring, cv=5, n_jobs=-1, error_score='raise').mean()  # cross_val_score也是测试集分数\n",
    "        train_scores.append(train_score)\n",
    "        test_score = cross_val_score(model, X_test[cols], y_test, scoring=scoring, cv=5, n_jobs=-1, error_score='raise').mean()  # cross_val_score也是测试集分数\n",
    "        test_scores.append(test_score)\n",
    "        oot_score = cross_val_score(model, X_oot[cols], y_oot, scoring=scoring, cv=5, n_jobs=-1, error_score='raise').mean()  # cross_val_score也是测试集分数\n",
    "        oot_scores.append(oot_score)\n",
    "\n",
    "    scores = pd.DataFrame([])\n",
    "    scores['train'] = train_scores\n",
    "    scores['test'] = test_scores\n",
    "    scores['oot'] = oot_scores\n",
    "    return feature_ranking, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 嵌入法  \n",
    "SelectFromModel没有RFE靠谱  \n",
    "RFE迭代删除最不重要的特征  \n",
    "SelectFromModel 的鲁棒性稍差一些，因为它只是根据作为参数给出的阈值删除不太重要的特征。不涉及迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_embedded(clf, X, y, scoring='roc_auc', threshold=[]):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "    #! 画学习曲线来找最佳阈值（特征重要性）\n",
    "    if len(threshold) == 0:\n",
    "        threshold = np.linspace(\n",
    "            0, (clf.fit(X, y).feature_importances_).max(), 20)\n",
    "    scores = []\n",
    "    for th in threshold:\n",
    "        X_embedded = SelectFromModel(clf, threshold=th).fit_transform(X, y)\n",
    "        # score = cross_val_score(xgbc, X_embedded, y, cv=5).mean()\n",
    "        # scores.append(score)\n",
    "        score = cross_validate(\n",
    "            clf, X_embedded, y, scoring=scoring, cv=5, n_jobs=-1, error_score='raise')\n",
    "        scores.append(score['test_score'].mean())\n",
    "    plt.plot(threshold, scores)\n",
    "    plt.show()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 线性降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多重共线性分析  \n",
    "特征变量与其他特征变量之间的相关性系数大，存在较大的共线性影响，会导致模型不准确。可使用降维的方法，去除多重共线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vif(train_data):\n",
    "\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "    # 方差膨胀因子VIF：如果将i特征添加到线性回归中，则参数估计的方差增加的度量\n",
    "    # 如果VIF大于5，则i特征与其他特征高度共线，因此参数估计会有很大的标准误差\n",
    "    X = np.matrix(train_data.values)\n",
    "    VIF_list = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    return VIF_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# 通过某种线性投影，将高维数据映射到低维空间，并期望在所投影的维度上，数据的方差最大，以达到用较少的数据也能保留较多原数据特性的效果\n",
    "PCA(n_components=0.9)  # 保持90%的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性判别分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# PCA是尽可能保留原数据信息，\n",
    "# LDA是使降维后的数据尽可能的区分开，其利用了标签的信息，使同类的数据尽可能接近，不同类数据尽可能分开\n",
    "LDA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过程监控"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习曲线  \n",
    "X轴：训练集样本数，y轴：模型准确率/回归误差，帮助判断方差/偏差是否过高，增大训练集是否减小过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, scoring='roc_auc', ax=None, random_state=2022):\n",
    "    from sklearn.model_selection import learning_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, scoring=scoring, shuffle=True,\n",
    "        cv=5, random_state=random_state, n_jobs=-1)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0.0, 1.1)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid()  # 绘制网格，不是必须\n",
    "    ax.fill_between(\n",
    "        train_sizes, train_scores_mean - train_scores_std, \n",
    "        train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "    ax.fill_between(\n",
    "        train_sizes, test_scores_mean - test_scores_std, \n",
    "        test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1),\n",
    "            'o-', color=\"r\", label=\"Training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1),\n",
    "            'o-', color=\"g\", label=\"Test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证曲线  \n",
    "X轴：超参数值，y轴：模型评分，随着超参数变化，模型的变化（比如从欠拟合——合适——过拟合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(estimator, title, X, y, param_name='alpha', param_range=[0.1, 0.01, 0.001, 0.0001], scoring='roc_auc'):\n",
    "    from sklearn.model_selection import validation_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, param_name=param_name, param_range=param_range, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.semilogx(param_range, train_scores_mean, label='Train Score', color='r')\n",
    "    plt.fill_between(\n",
    "        param_range, train_scores_mean - train_scores_std, \n",
    "        train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "    plt.semilogx(param_range, test_scores_mean, label='Cross-validation Score', color='g')\n",
    "    plt.fill_between(\n",
    "        param_range, test_scores_mean - test_scores_std, \n",
    "        test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "    # plt.plot(param_range, train_scores_mean, 'o-', color='r', label='Train Score')\n",
    "    # plt.plot(param_range, test_scores_mean, 'o-', color='g', label='Cross-validation Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bagging  \n",
    "1. 先Bootstrap sampling，得到N个包含m个样本的dataset\n",
    "2. 再训练多个基模型。如随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### blending\n",
    "1. 第一层：\n",
    "- 把Data Set（假设shape=(100, D)）分为两部分，80%作训练集train，20%作测试集test\n",
    "- 再次把train按5:5分为两部分train1、train2，用train1训练N个模型\n",
    "- 每一个模型对train2、和test作预测，得到(40, N)和(20, N)\n",
    "- 再将预测值合并到train2和test，得到(40, D+N)和(20, D+N)\n",
    "2. 第二层：\n",
    "- 用得到的新train2(40, D+N)训练模型（或直接用（40, N）训练，（20, N）测试）\n",
    "- 用(20, D+N)来测试模型预测值与y的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：\n",
    "- blending不用做k折交叉验证，获得新的feature  \n",
    "- 避免一些信息泄漏  \n",
    "\n",
    "缺点：\n",
    "- 得到最终模型第二层，样本太少（按7:3划分train和test，再按7:3划分train1和train2，则第二层训练集只有9%的样本）\n",
    "- 很可能过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "\n",
    "1. 第一步：\n",
    "- 训练3个基模型，用所有基模型对样本进行预测。第j个基模型对样本i的预测值或矩阵=新的dataset.iloc[i, j]\n",
    "- 每一个模型：五折交叉验证，训练集前4折训练，第五折验证，model给出第五折验证集的预测值$pred_5$和完整测试集$pred_{test5}$，五折走完得到$pred_1 \\dots pred_5$堆叠形成训练集的$train$，对测试集的$pred_{test1} \\dots pred_{test5}$按行取平均，得到测试集的$test$\n",
    "2. 第二步：基于新的训练集训练模型\n",
    "3个模型训练完后，得到$\\begin{pmatrix}  \n",
    "  train_{1} & train_{2} & train_{3} \\\\  \n",
    "  test_{1} & test_{2} & test_{3}\n",
    "\\end{pmatrix} $，使用train训练第二轮模型，使用test测试第二轮模型的效果model2(test)与y的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20210513224705277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM2NTQ0Mw==,size_16,color_FFFFFF,t_70\" style=\"zoom:50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=2022)\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42)))]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=rkf, stack_method='auto', n_jobs=-1, passthrough=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missrate_by_month(data, month_col, x_cols):\n",
    "    df = data.groupby(month_col)[x_cols].apply(\n",
    "    lambda x: x.isna().sum() / len(x))\n",
    "    df = df.T\n",
    "    df['miss_rate_d'] = df.std(axis=1)\n",
    "    return df\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "[statsmodels.stats.outliers_influence.variance_inflation_factor(df.values,i) for i in range(df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variation_by_month(df, time_col, columns, label_col):\n",
    "    variation_dic = {}\n",
    "    for col in columns:\n",
    "        feature_v = df.pivot_table(index=col, columns=time_col, values=label_col, agg_func=['mean'])\n",
    "        variation_dic[col] = feature_v.rank().std(axis=1).mean()\n",
    "    return pd.DataFrame([variation_dic], index=['variation']).T  # 设置阈值，如波动值<0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
